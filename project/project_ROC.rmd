---
title: "Pokemon Project"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(caret)
```

## Data
--- add description here

## Feature extraction
Import the df and visualize the columns' name.
```{r}
df <- read.csv("data.csv")
names(df)
```
Drop useless columns X., Name.

```{r}
df <- df |>
    mutate(Legendary = as.integer(as.logical(Legendary))) |>
    select(-X., -Name)
```
Check that Tot is just a linear combination of other columns. If yes, drop it.
```{r}
if (all((df$HP + df$Attack + df$Defense + df$SP..Attack + df$SP..Defense + df$Speed) == df$Total)) {
    df <- df |> select(-Total)
}
```

Hot-encoding from Type.1 and Type.2.

```{r}
unique_types <- c(df$Type.1, df$Type.2) |> unique()

for (typ in unique_types[-1]) {
    if (typ == "") next
    df[typ] <- 0
    for (row in 1:nrow(df)) {
        has_type <- typ %in% df[row, c("Type.1", "Type.2")]
        if (has_type) {
            df[row, typ] <- 1
        }
    }
}
```
Encode Generation.
```{r}
for (i in unique(df$Generation)) {
    if (i == 1) next
    col_name <- paste0("gen_", i)
    df[col_name] <- 0
}

for (row in 1:nrow(df)) {
    gen <- df[row, "Generation"]
    if (gen == 1) next
    col_name <- paste0("gen_", gen)
    df[row, col_name] <- 1
}
```
Drop the categorical columns that we don't need anymore and set Legendary to factor.
```{r}
df <- df |>
    select(-Type.1, -Type.2, -Generation) |>
    mutate(Legendary = as.factor(ifelse(Legendary == 0, "No", "Yes")))
```

```{r}
colnames(df)[c(1, 4, 5)] <- c("Hit_Point", "Special_Attack", "Special_Defense")
```

```{r}
set.seed(123)
train_test_split <- function(df, perc_train = 0.7) {
    i_train <- sample(1:nrow(df), floor(0.7 * nrow(df)), F)
    list_out <- list(train = df[i_train, ], test = df[-i_train, ])
    return(list_out)
}
df_split <- train_test_split(df)
```

```{r}
train <- df_split$train
test <- df_split$test
```

```{r}
numerical_vars <- names(train)[1:6]
```
Verify whether the prob of having Legendary is equal accross Generation to decide whether to keep the variable

```{r}
# train %>%
#    group_by(Generation) %>%
#    summarise(mean(Legendary))
```


Categorical data
```{r}
train %>%
    select(-numerical_vars, -Legendary) %>%
    colMeans() * 100 # Percentage values
```

```{r}
# Numerical Variables
train %>%
    select(numerical_vars) %>%
    summary()
```

```{r}
par(mfrow = c(2, 4), mar = c(3, 3, 3, 3))
lapply(numerical_vars, function(col_name) {
    hist(train[[col_name]], main = paste("Histogram of", col_name), xlab = "variable")
})
par(mfrow = c(1, 1))
```

```{r}
library(corrplot)
cor_mat <- cor(train[numerical_vars])
print(cor_mat)
corrplot(cor_mat, method = "square", type = "upper", tl.col = "black", tl.srt = 45)
```



```{r}
train_std <- train
train_std[numerical_vars] <- train |>
    select(numerical_vars) |>
    mutate(across(where(is.numeric), scale))

# Apply the same scaling to test data
test_std <- test
test_std[numerical_vars] <- test |>
    select(numerical_vars) |>
    mutate(across(where(is.numeric), scale))
```
Checks
```{r}
sapply(list(mean = mean, sd = sd), mapply, train_std |> select(numerical_vars))
sapply(list(mean = mean, sd = sd), mapply, test_std |> select(numerical_vars))
```

```{r}
extract_best_roc <- function(fit) fit$results[which.max(fit$results$ROC), ]
```
Fitting a logistic elastic net. Grid search of optimal alpha and lambda. CV. Maximize ROC.
```{r}
grid_ <- expand.grid(
    .alpha = seq(0, 1, by = 0.05),
    .lambda = seq(0, 0.2, by = 0.01)
)

train_control_1 <- trainControl(
    method = "cv",
    number = 10,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
)

logistic_elnet_1 <- train(
    Legendary ~ .,
    data = train_std,
    method = "glmnet",
    family = "binomial",
    metric = "ROC",
    tuneGrid = grid_,
    trControl = train_control_1
)

print(logistic_elnet_1$bestTune)
print(extract_best_roc(logistic_elnet_1))
plot(logistic_elnet_1)
```

Fitting a logistic elastic net. Grid search of optimal alpha and lambda. SMOTE CV. Maximize ROC.
```{r}
train_control_2 <- trainControl(
    method = "cv",
    number = 10,
    sampling = "smote",
    classProbs = TRUE,
    summaryFunction = twoClassSummary
)

logistic_elnet_2 <- train(
    Legendary ~ .,
    data = train_std,
    method = "glmnet",
    family = "binomial",
    metric = "ROC",
    tuneGrid = grid_,
    trControl = train_control_2
)
print(logistic_elnet_2$bestTune)
print(extract_best_roc(logistic_elnet_2))
plot(logistic_elnet_2)
```

Fitting a logistic elastic net. Grid search of optimal alpha and lambda. Upsampling CV. Maximize ROC.

```{r}
train_control_3 <- trainControl(
    method = "cv",
    number = 10,
    sampling = "up",
    classProbs = TRUE,
    summaryFunction = twoClassSummary
)

logistic_elnet_3 <- train(
    Legendary ~ .,
    data = train_std,
    method = "glmnet",
    family = "binomial",
    metric = "ROC",
    tuneGrid = grid_,
    trControl = train_control_3
)
print(logistic_elnet_3$bestTune)
print(extract_best_roc(logistic_elnet_3))
plot(logistic_elnet_3)
```

Fitting a logistic elastic net. Grid search of optimal alpha and lambda. Downsampling CV. Maximize ROC.

```{r}
train_control_4 <- trainControl(
    method = "cv",
    number = 10,
    sampling = "down",
    classProbs = TRUE,
    summaryFunction = twoClassSummary
)

logistic_elnet_4 <- train(
    Legendary ~ .,
    data = train_std,
    method = "glmnet",
    family = "binomial",
    metric = "ROC",
    tuneGrid = grid_,
    trControl = train_control_3
)
print(logistic_elnet_4$bestTune)
print(extract_best_roc(logistic_elnet_4))
plot(logistic_elnet_4)
```

### Prediction
```{r}
get_prediction <- function(fit, type = "raw") {
    predict(fit, type = type, newdata = test_std |> select(-Legendary))
}

get_accuracy <- function(fit) {
    y <- test_std$Legendary
    y_hat <- predict(fit, type = "raw", newdata = test_std |>
        select(-Legendary))
    return(mean(y == y_hat))
}
```
```{r}
lapply(list(logistic_elnet_1, logistic_elnet_2, logistic_elnet_3, logistic_elnet_4), get_accuracy)
```

```{r}
# test_std <- test_std %>% select(-Legendary)
# best_elnet_pred <- predict(logistic_elnet, newx = test_std, type = "raw")
# print(best_elnet_pred)
```
Now we check:
```{r}
# test$Legendary <- as.factor(test$Legendary)
# levels(test$Legendary) <- c("No", "Yes")
# confusionMatrix(best_elnet_pred, test$Legendary)

# nrow(train_std)
# nrow(test_std)
# length(best_elnet_pred)
```



